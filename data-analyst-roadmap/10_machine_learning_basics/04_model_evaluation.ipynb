{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Evaluation\n",
                "\n",
                "Measure and improve model performance.\n",
                "\n",
                "## Why Evaluate Models?\n",
                "- Measure accuracy\n",
                "- Compare models\n",
                "- Detect overfitting\n",
                "- Optimize performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import (\n",
                "    train_test_split,\n",
                "    cross_val_score,\n",
                "    learning_curve\n",
                ")\n",
                "from sklearn.linear_model import (\n",
                "    LinearRegression,\n",
                "    LogisticRegression\n",
                ")\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import *\n",
                "\n",
                "sns.set_style('whitegrid')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Regression Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate sample data\n",
                "np.random.seed(42)\n",
                "X = np.random.rand(100, 1) * 10\n",
                "y_true = 2.5 * X + 5 + np.random.randn(100, 1) * 2\n",
                "\n",
                "# Train model\n",
                "model = LinearRegression()\n",
                "model.fit(X, y_true)\n",
                "y_pred = model.predict(X)\n",
                "\n",
                "print(\"Regression model trained!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics\n",
                "mae = mean_absolute_error(y_true, y_pred)\n",
                "mse = mean_squared_error(y_true, y_pred)\n",
                "rmse = np.sqrt(mse)\n",
                "r2 = r2_score(y_true, y_pred)\n",
                "\n",
                "print(\"Regression Metrics:\")\n",
                "print(f\"MAE:  {mae:.3f}\")\n",
                "print(f\"MSE:  {mse:.3f}\")\n",
                "print(f\"RMSE: {rmse:.3f}\")\n",
                "print(f\"R²:   {r2:.3f}\")\n",
                "\n",
                "print(\"\\nInterpretation:\")\n",
                "print(f\"- On average, predictions are off by \"\n",
                "      f\"{mae:.2f} units (MAE)\")\n",
                "print(f\"- Model explains {r2*100:.1f}% of \"\n",
                "      f\"variance (R²)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Classification Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load iris dataset\n",
                "from sklearn.datasets import load_iris\n",
                "\n",
                "iris = load_iris()\n",
                "X = iris.data\n",
                "y = iris.target\n",
                "\n",
                "# Binary classification (class 0 vs rest)\n",
                "y_binary = (y == 0).astype(int)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y_binary, test_size=0.3, random_state=42\n",
                ")\n",
                "\n",
                "# Train model\n",
                "clf = LogisticRegression(max_iter=200)\n",
                "clf.fit(X_train, y_train)\n",
                "y_pred = clf.predict(X_test)\n",
                "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
                "\n",
                "print(\"Classification model trained!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
                "plt.ylabel('Actual')\n",
                "plt.xlabel('Predicted')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.show()\n",
                "\n",
                "# Extract values\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "print(f\"True Negatives:  {tn}\")\n",
                "print(f\"False Positives: {fp}\")\n",
                "print(f\"False Negatives: {fn}\")\n",
                "print(f\"True Positives:  {tp}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate metrics\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "precision = precision_score(y_test, y_pred)\n",
                "recall = recall_score(y_test, y_pred)\n",
                "f1 = f1_score(y_test, y_pred)\n",
                "\n",
                "print(\"Classification Metrics:\")\n",
                "print(f\"Accuracy:  {accuracy:.3f}\")\n",
                "print(f\"Precision: {precision:.3f}\")\n",
                "print(f\"Recall:    {recall:.3f}\")\n",
                "print(f\"F1 Score:  {f1:.3f}\")\n",
                "\n",
                "print(\"\\nInterpretation:\")\n",
                "print(f\"- {accuracy*100:.1f}% of predictions \"\n",
                "      f\"are correct\")\n",
                "print(f\"- {precision*100:.1f}% of positive \"\n",
                "      f\"predictions are correct\")\n",
                "print(f\"- {recall*100:.1f}% of actual positives \"\n",
                "      f\"were found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ROC Curve and AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate ROC curve\n",
                "fpr, tpr, thresholds = roc_curve(\n",
                "    y_test, \n",
                "    y_pred_proba\n",
                ")\n",
                "auc = roc_auc_score(y_test, y_pred_proba)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(\n",
                "    fpr, tpr, \n",
                "    linewidth=2, \n",
                "    label=f'Model (AUC = {auc:.3f})'\n",
                ")\n",
                "plt.plot(\n",
                "    [0, 1], [0, 1], \n",
                "    'k--', \n",
                "    label='Random'\n",
                ")\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curve')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "print(f\"AUC Score: {auc:.3f}\")\n",
                "print(\"\\nInterpretation:\")\n",
                "if auc > 0.9:\n",
                "    print(\"Excellent model!\")\n",
                "elif auc > 0.8:\n",
                "    print(\"Good model\")\n",
                "elif auc > 0.7:\n",
                "    print(\"Fair model\")\n",
                "else:\n",
                "    print(\"Poor model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Cross-Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5-fold cross-validation\n",
                "scores = cross_val_score(\n",
                "    clf, X, y_binary, \n",
                "    cv=5, \n",
                "    scoring='accuracy'\n",
                ")\n",
                "\n",
                "print(\"Cross-Validation Scores:\")\n",
                "for i, score in enumerate(scores, 1):\n",
                "    print(f\"Fold {i}: {score:.3f}\")\n",
                "\n",
                "print(f\"\\nMean: {scores.mean():.3f}\")\n",
                "print(f\"Std:  {scores.std():.3f}\")\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(8, 5))\n",
                "plt.bar(range(1, 6), scores)\n",
                "plt.axhline(\n",
                "    y=scores.mean(), \n",
                "    color='r', \n",
                "    linestyle='--',\n",
                "    label=f'Mean: {scores.mean():.3f}'\n",
                ")\n",
                "plt.xlabel('Fold')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title('Cross-Validation Scores')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Learning Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate learning curve\n",
                "train_sizes, train_scores, val_scores = learning_curve(\n",
                "    clf, X, y_binary,\n",
                "    cv=5,\n",
                "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
                "    scoring='accuracy'\n",
                ")\n",
                "\n",
                "# Calculate means and stds\n",
                "train_mean = train_scores.mean(axis=1)\n",
                "train_std = train_scores.std(axis=1)\n",
                "val_mean = val_scores.mean(axis=1)\n",
                "val_std = val_scores.std(axis=1)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(\n",
                "    train_sizes, train_mean, \n",
                "    label='Training score',\n",
                "    linewidth=2\n",
                ")\n",
                "plt.fill_between(\n",
                "    train_sizes,\n",
                "    train_mean - train_std,\n",
                "    train_mean + train_std,\n",
                "    alpha=0.2\n",
                ")\n",
                "plt.plot(\n",
                "    train_sizes, val_mean,\n",
                "    label='Validation score',\n",
                "    linewidth=2\n",
                ")\n",
                "plt.fill_between(\n",
                "    train_sizes,\n",
                "    val_mean - val_std,\n",
                "    val_mean + val_std,\n",
                "    alpha=0.2\n",
                ")\n",
                "plt.xlabel('Training Set Size')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title('Learning Curves')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare multiple models\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(\n",
                "        max_iter=200\n",
                "    ),\n",
                "    'Decision Tree': DecisionTreeClassifier(\n",
                "        random_state=42\n",
                "    ),\n",
                "    'Random Forest': RandomForestClassifier(\n",
                "        random_state=42\n",
                "    )\n",
                "}\n",
                "\n",
                "results = []\n",
                "\n",
                "for name, model in models.items():\n",
                "    scores = cross_val_score(\n",
                "        model, X, y_binary, \n",
                "        cv=5, \n",
                "        scoring='accuracy'\n",
                "    )\n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Mean': scores.mean(),\n",
                "        'Std': scores.std()\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df = results_df.sort_values(\n",
                "    'Mean', \n",
                "    ascending=False\n",
                ")\n",
                "results_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize comparison\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.barh(\n",
                "    results_df['Model'], \n",
                "    results_df['Mean']\n",
                ")\n",
                "plt.xlabel('Mean Accuracy')\n",
                "plt.title('Model Comparison')\n",
                "plt.xlim(0.8, 1.0)\n",
                "plt.grid(axis='x')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Overfitting vs Underfitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate overfitting\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "depths = range(1, 20)\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "\n",
                "for depth in depths:\n",
                "    dt = DecisionTreeClassifier(\n",
                "        max_depth=depth, \n",
                "        random_state=42\n",
                "    )\n",
                "    dt.fit(X_train, y_train)\n",
                "    \n",
                "    train_scores.append(\n",
                "        dt.score(X_train, y_train)\n",
                "    )\n",
                "    test_scores.append(\n",
                "        dt.score(X_test, y_test)\n",
                "    )\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(\n",
                "    depths, train_scores, \n",
                "    label='Training',\n",
                "    linewidth=2\n",
                ")\n",
                "plt.plot(\n",
                "    depths, test_scores, \n",
                "    label='Testing',\n",
                "    linewidth=2\n",
                ")\n",
                "plt.xlabel('Tree Depth')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title('Overfitting Example')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "print(\"Interpretation:\")\n",
                "print(\"- Gap between train and test = overfitting\")\n",
                "print(\"- Both low = underfitting\")\n",
                "print(\"- Both high = good fit\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Practice Exercises"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 1\n",
                "Calculate precision, recall, and F1 score \n",
                "for a custom threshold."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2\n",
                "Compare 3 different models using \n",
                "cross-validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "✅ **Regression** - MAE, MSE, RMSE, R²  \n",
                "✅ **Classification** - Accuracy, Precision, Recall, F1  \n",
                "✅ **ROC/AUC** - Overall performance  \n",
                "✅ **Cross-Validation** - Robust evaluation  \n",
                "✅ **Learning Curves** - Detect over/underfitting  \n",
                "\n",
                "**Next:** [Feature Engineering](05_feature_engineering.ipynb) →"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}